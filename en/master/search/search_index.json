{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"OneFlow: A Whole New Generation Of Deep Learning Framework \u00b6 An open source deep learning framework with whole new frame design and the world's leading technology for distributed system. Key Features & Capabilities \u00b6 Make distributed training with multi-machines and multi-devices so simple as on single device. Perfectly support container platforms(k8s & docker) Handle large models easily Almost zero runtime overhead & linear speedup Support multiple deep learning compilers(XLA, TensorRT etc) Support automatic mixed precision Will support more operators and models sustainsouly etc. We are trying to build a deep learning framework that will amaze everyone! Looking forward to your feedbacks and welcome to join our contributor community . Get Started \u00b6 Follow the instructions to install OneFlow. Tackle the common tasks of machine learning with OneFlow in Basic topics . Such as building network, hyper-parameters configuration, loading data, distributed training and so on. If you want to know more about the characteristics of OneFlow, such as the format of OneFlow's dataset, the parallelism view of OneFlow or how to debug OneFlow framework with vscode, please refer to extended topic . We highly expect developers and geeks to join our contributor community . Together we can build a perfect deep learning framework.","title":"Home"},{"location":"index.html#oneflow-a-whole-new-generation-of-deep-learning-framework","text":"An open source deep learning framework with whole new frame design and the world's leading technology for distributed system.","title":"OneFlow: A Whole New Generation Of Deep Learning Framework"},{"location":"index.html#key-features-capabilities","text":"Make distributed training with multi-machines and multi-devices so simple as on single device. Perfectly support container platforms(k8s & docker) Handle large models easily Almost zero runtime overhead & linear speedup Support multiple deep learning compilers(XLA, TensorRT etc) Support automatic mixed precision Will support more operators and models sustainsouly etc. We are trying to build a deep learning framework that will amaze everyone! Looking forward to your feedbacks and welcome to join our contributor community .","title":"Key Features &amp; Capabilities"},{"location":"index.html#get-started","text":"Follow the instructions to install OneFlow. Tackle the common tasks of machine learning with OneFlow in Basic topics . Such as building network, hyper-parameters configuration, loading data, distributed training and so on. If you want to know more about the characteristics of OneFlow, such as the format of OneFlow's dataset, the parallelism view of OneFlow or how to debug OneFlow framework with vscode, please refer to extended topic . We highly expect developers and geeks to join our contributor community . Together we can build a perfect deep learning framework.","title":"Get Started"},{"location":"basics_topics/async_get.html","text":"Get Results from Job Function \u00b6 In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously. Difference Between Synchronous and Asynchronous \u00b6 Synchronization \u00b6 During synchronous training, the training of the next step cannot be started until the work of the previous step is completed. Asynchronization \u00b6 In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different Get Result Synchronously \u00b6 When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy . Data Types in oneflow.typing \u00b6 The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case . Get Result Asynchronously \u00b6 Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function. Prepare a Callback Function \u00b6 Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function. Registration of Callback Function \u00b6 When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss ) Code \u00b6 Get single result synchronously \u00b6 We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved \u00b6 Get Mutiple Results Synchronously \u00b6 In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3% \u00b6 Get Single Result Asynchronously \u00b6 In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ... Get Multiple Results Asynchronously \u00b6 In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get results from job function"},{"location":"basics_topics/async_get.html#get-results-from-job-function","text":"In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously.","title":"Get Results from Job Function"},{"location":"basics_topics/async_get.html#difference-between-synchronous-and-asynchronous","text":"","title":"Difference Between Synchronous and Asynchronous"},{"location":"basics_topics/async_get.html#synchronization","text":"During synchronous training, the training of the next step cannot be started until the work of the previous step is completed.","title":"Synchronization"},{"location":"basics_topics/async_get.html#asynchronization","text":"In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different","title":"Asynchronization"},{"location":"basics_topics/async_get.html#get-result-synchronously","text":"When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy .","title":"Get Result Synchronously"},{"location":"basics_topics/async_get.html#data-types-in-oneflowtyping","text":"The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case .","title":"Data Types in oneflow.typing"},{"location":"basics_topics/async_get.html#get-result-asynchronously","text":"Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function.","title":"Get Result Asynchronously"},{"location":"basics_topics/async_get.html#prepare-a-callback-function","text":"Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function.","title":"Prepare a Callback Function"},{"location":"basics_topics/async_get.html#registration-of-callback-function","text":"When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss )","title":"Registration of Callback Function"},{"location":"basics_topics/async_get.html#code","text":"","title":"Code"},{"location":"basics_topics/async_get.html#get-single-result-synchronously","text":"We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved","title":"Get single result synchronously"},{"location":"basics_topics/async_get.html#_1","text":"","title":""},{"location":"basics_topics/async_get.html#get-mutiple-results-synchronously","text":"In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3%","title":"Get Mutiple Results Synchronously"},{"location":"basics_topics/async_get.html#_2","text":"","title":""},{"location":"basics_topics/async_get.html#get-single-result-asynchronously","text":"In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ...","title":"Get Single Result Asynchronously"},{"location":"basics_topics/async_get.html#get-multiple-results-asynchronously","text":"In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get Multiple Results Asynchronously"},{"location":"basics_topics/build_nn_with_op_and_layer.html","text":"Build a Neural Network \u00b6 In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input. Operator and Layer \u00b6 Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators. Data block in neural network \u00b6 OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype ) Operator Overloading \u00b6 The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases) Summary \u00b6 Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Build a Neural Network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#build-a-neural-network","text":"In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input.","title":"Build a Neural Network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#operator-and-layer","text":"Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators.","title":"Operator and Layer"},{"location":"basics_topics/build_nn_with_op_and_layer.html#data-block-in-neural-network","text":"OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype )","title":"Data block in neural network"},{"location":"basics_topics/build_nn_with_op_and_layer.html#operator-overloading","text":"The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases)","title":"Operator Overloading"},{"location":"basics_topics/build_nn_with_op_and_layer.html#summary","text":"Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Summary"},{"location":"basics_topics/concept_explanation.html","text":"Term & Concept in OneFlow \u00b6 In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts. Algorithms Development \u00b6 1.Placeholder \u00b6 Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 . 2.Tensor and Blob \u00b6 Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time. 3.Job Function \u00b6 In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources. 4.Layer and Operator \u00b6 Layer \u00b6 The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable . Operator \u00b6 Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators. 5.Mirrored View and Consistent View \u00b6 OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler. Framework developing \u00b6 1.Boxing \u00b6 The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission. 2.SBP \u00b6 All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum. Split \u00b6 In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis. Broadcast \u00b6 In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device. Partial Sum \u00b6 If an operator has distributive property, different part of tensor can be simply added. 3.TensorBuffer and TensorList \u00b6 Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList . TensorBuffer \u00b6 TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous . TensorList \u00b6 Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"Term & Concept Explanation"},{"location":"basics_topics/concept_explanation.html#term-concept-in-oneflow","text":"In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts.","title":"Term &amp; Concept in OneFlow"},{"location":"basics_topics/concept_explanation.html#algorithms-development","text":"","title":"Algorithms Development"},{"location":"basics_topics/concept_explanation.html#1placeholder","text":"Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 .","title":"1.Placeholder"},{"location":"basics_topics/concept_explanation.html#2tensor-and-blob","text":"Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time.","title":"2.Tensor and Blob"},{"location":"basics_topics/concept_explanation.html#3job-function","text":"In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources.","title":"3.Job Function"},{"location":"basics_topics/concept_explanation.html#4layer-and-operator","text":"","title":"4.Layer and Operator"},{"location":"basics_topics/concept_explanation.html#layer","text":"The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable .","title":"Layer"},{"location":"basics_topics/concept_explanation.html#operator","text":"Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators.","title":"Operator"},{"location":"basics_topics/concept_explanation.html#5mirrored-view-and-consistent-view","text":"OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler.","title":"5.Mirrored View and Consistent View"},{"location":"basics_topics/concept_explanation.html#framework-developing","text":"","title":"Framework developing"},{"location":"basics_topics/concept_explanation.html#1boxing","text":"The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission.","title":"1.Boxing"},{"location":"basics_topics/concept_explanation.html#2sbp","text":"All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum.","title":"2.SBP"},{"location":"basics_topics/concept_explanation.html#split","text":"In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis.","title":"Split"},{"location":"basics_topics/concept_explanation.html#broadcast","text":"In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device.","title":"Broadcast"},{"location":"basics_topics/concept_explanation.html#partial-sum","text":"If an operator has distributive property, different part of tensor can be simply added.","title":"Partial Sum"},{"location":"basics_topics/concept_explanation.html#3tensorbuffer-and-tensorlist","text":"Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList .","title":"3.TensorBuffer and TensorList"},{"location":"basics_topics/concept_explanation.html#tensorbuffer","text":"TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous .","title":"TensorBuffer"},{"location":"basics_topics/concept_explanation.html#tensorlist","text":"Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"TensorList"},{"location":"basics_topics/data_input.html","text":"Data Input \u00b6 Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow. Thus we recommend use that in mature projects. Use Numpy as Data Input \u00b6 Example \u00b6 We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , ) Code Explanation \u00b6 In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function . Using DataLoader and Related Operators \u00b6 Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details. Examples \u00b6 The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to this article . Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,) Code Explanation \u00b6 There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image. More Formats Support by DataLoader \u00b6 OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future.","title":"Data Input"},{"location":"basics_topics/data_input.html#data-input","text":"Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow. Thus we recommend use that in mature projects.","title":"Data Input"},{"location":"basics_topics/data_input.html#use-numpy-as-data-input","text":"","title":"Use Numpy as Data Input"},{"location":"basics_topics/data_input.html#example","text":"We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , )","title":"Example"},{"location":"basics_topics/data_input.html#code-explanation","text":"In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function .","title":"Code Explanation"},{"location":"basics_topics/data_input.html#using-dataloader-and-related-operators","text":"Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details.","title":"Using DataLoader and Related Operators"},{"location":"basics_topics/data_input.html#examples","text":"The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to this article . Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,)","title":"Examples"},{"location":"basics_topics/data_input.html#code-explanation_1","text":"There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image.","title":"Code Explanation"},{"location":"basics_topics/data_input.html#more-formats-support-by-dataloader","text":"OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future.","title":"More Formats Support by DataLoader"},{"location":"basics_topics/distributed_train.html","text":"Distributed Training \u00b6 In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow. The Distribution Advantage of OneFlow. \u00b6 OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system. Configuration of the Distributed Training \u00b6 By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations. Single Machine Program \u00b6 Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #... Configuration of Ports and Device \u00b6 In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 ) Node Configuration \u00b6 Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n Complete Code of Distributed Training \u00b6 After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/master/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 . FAQ \u00b6 After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"Distributed training"},{"location":"basics_topics/distributed_train.html#distributed-training","text":"In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow.","title":"Distributed Training"},{"location":"basics_topics/distributed_train.html#the-distribution-advantage-of-oneflow","text":"OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system.","title":"The Distribution Advantage of OneFlow."},{"location":"basics_topics/distributed_train.html#configuration-of-the-distributed-training","text":"By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations.","title":"Configuration of the Distributed Training"},{"location":"basics_topics/distributed_train.html#single-machine-program","text":"Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #...","title":"Single Machine Program"},{"location":"basics_topics/distributed_train.html#configuration-of-ports-and-device","text":"In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 )","title":"Configuration of Ports and Device"},{"location":"basics_topics/distributed_train.html#node-configuration","text":"Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n","title":"Node Configuration"},{"location":"basics_topics/distributed_train.html#complete-code-of-distributed-training","text":"After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/master/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 .","title":"Complete Code of Distributed Training"},{"location":"basics_topics/distributed_train.html#faq","text":"After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"FAQ"},{"location":"basics_topics/essentials_of_oneflow.html","text":"OneFlow System Design \u00b6 In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary Motivation \u00b6 OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios. Runtime based on actor system \u00b6 Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing. Decentralized scheduling \u00b6 OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism. Pipelining \u00b6 In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system. Data movement as a first-class citizen \u00b6 In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct. Exploit parallelism as much as possible \u00b6 In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication. Compile-time based on a formal description of parallelism \u00b6 OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world. Placement \u00b6 While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism SBP \u00b6 SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization. The most user-friendly distributed framework \u00b6 OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations. Summary \u00b6 In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"OneFlow System Design"},{"location":"basics_topics/essentials_of_oneflow.html#oneflow-system-design","text":"In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary","title":"OneFlow System Design"},{"location":"basics_topics/essentials_of_oneflow.html#motivation","text":"OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios.","title":"Motivation"},{"location":"basics_topics/essentials_of_oneflow.html#runtime-based-on-actor-system","text":"Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing.","title":"Runtime based on actor system"},{"location":"basics_topics/essentials_of_oneflow.html#decentralized-scheduling","text":"OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism.","title":"Decentralized scheduling"},{"location":"basics_topics/essentials_of_oneflow.html#pipelining","text":"In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system.","title":"Pipelining"},{"location":"basics_topics/essentials_of_oneflow.html#data-movement-as-a-first-class-citizen","text":"In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct.","title":"Data movement as a first-class citizen"},{"location":"basics_topics/essentials_of_oneflow.html#exploit-parallelism-as-much-as-possible","text":"In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication.","title":"Exploit parallelism as much as possible"},{"location":"basics_topics/essentials_of_oneflow.html#compile-time-based-on-a-formal-description-of-parallelism","text":"OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world.","title":"Compile-time based on a formal description of parallelism"},{"location":"basics_topics/essentials_of_oneflow.html#placement","text":"While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism","title":"Placement"},{"location":"basics_topics/essentials_of_oneflow.html#sbp","text":"SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization.","title":"SBP"},{"location":"basics_topics/essentials_of_oneflow.html#the-most-user-friendly-distributed-framework","text":"OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations.","title":"The most user-friendly distributed framework"},{"location":"basics_topics/essentials_of_oneflow.html#summary","text":"In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"Summary"},{"location":"basics_topics/model_load_save.html","text":"Loading and Saving of Model \u00b6 For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model Use get_variable to Create/Obtain Model Parameters Object \u00b6 We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally. Process \u00b6 The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return. Use get_variable Create Object \u00b6 The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #... Initializer Setting \u00b6 In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer The Python Interface of OneFlow Models \u00b6 We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] Model Saving and Loading \u00b6 We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth. The Structure of OneFlow Saved Model \u00b6 OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job . Model Finetune and Transfer Learning \u00b6 In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow. Codes \u00b6 The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Loading and saving of model"},{"location":"basics_topics/model_load_save.html#loading-and-saving-of-model","text":"For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model","title":"Loading and Saving of Model"},{"location":"basics_topics/model_load_save.html#use-get_variable-to-createobtain-model-parameters-object","text":"We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally.","title":"Use get_variable to Create/Obtain Model Parameters Object"},{"location":"basics_topics/model_load_save.html#process","text":"The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return.","title":"Process"},{"location":"basics_topics/model_load_save.html#use-get_variable-create-object","text":"The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #...","title":"Use get_variable Create Object"},{"location":"basics_topics/model_load_save.html#initializer-setting","text":"In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer","title":"Initializer Setting"},{"location":"basics_topics/model_load_save.html#the-python-interface-of-oneflow-models","text":"We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]","title":"The Python Interface of OneFlow Models"},{"location":"basics_topics/model_load_save.html#model-saving-and-loading","text":"We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth.","title":"Model Saving and Loading"},{"location":"basics_topics/model_load_save.html#the-structure-of-oneflow-saved-model","text":"OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job .","title":"The Structure of OneFlow Saved Model"},{"location":"basics_topics/model_load_save.html#model-finetune-and-transfer-learning","text":"In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow.","title":"Model Finetune and Transfer Learning"},{"location":"basics_topics/model_load_save.html#codes","text":"The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Codes"},{"location":"basics_topics/optimizer_in_function_config.html","text":"Configuration of Optimization Algorithms and Hyperparameters \u00b6 After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api Job Function Configuration \u00b6 In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls . Example of Configurations \u00b6 Configuration for prediction/inference \u00b6 Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here Configuration for training \u00b6 If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation. FAQ \u00b6 Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"Optimization Algorithm and Parameter Configuration"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-of-optimization-algorithms-and-hyperparameters","text":"After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api","title":"Configuration of Optimization Algorithms and Hyperparameters"},{"location":"basics_topics/optimizer_in_function_config.html#job-function-configuration","text":"In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls .","title":"Job Function Configuration"},{"location":"basics_topics/optimizer_in_function_config.html#example-of-configurations","text":"","title":"Example of Configurations"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-for-predictioninference","text":"Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here","title":"Configuration for prediction/inference"},{"location":"basics_topics/optimizer_in_function_config.html#configuration-for-training","text":"If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation.","title":"Configuration for training"},{"location":"basics_topics/optimizer_in_function_config.html#faq","text":"Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"FAQ"},{"location":"contribute/intro.html","text":"As an open source infrastructure framework with prominent competitiveness and solid foundation, OneFlow is still in rapid development and improvement. OneFlow can't grow without the friends who love open source. Every line of code you contribute, every issue you submit, every bug you report, even every question you ask, is precious to OneFlow. Grow up with OneFlow \u00b6 Contributing code, adding API docstring, fixing bugs and submiting issues, reviewing code ... OneFlow looks forward to your help in every way. You can participate in OneFlow's open source project by choosing one or more of the following, depending on your situation. Contribute code Contribute test cases Improve documentation Review pull requests from others Recommend OneFlow to people who need it Participate in OneFlow's group discussion \u2026\u2026 All the things you can think of to make OneFlow better Contribute code \u00b6 Our main open source code repository are\uff1a oneflow : It contains codes of OneFlow framework. OneFlow-Benchmark : It provides OneFlow deep learning benchmark examples for CV, CTR and NLP, and more models are on the way and will be provided here when ready. You can also find or create more OneFlow open source projects here . Contribute test cases \u00b6 OneFlow has reproduced and tested a lot of popular models, some related scripts and documents are placed in OneFlow-Benchmark repository. But there are more models that are not covered by OneFlow, we are looking forward to your help to bring them to OneFlow. We also strongly encourage using OneFlow in your own original research and sharing it in OneFlow-Benchmark repository. Improve documentation \u00b6 The API documents of OneFlow are generated by sphinx to extract the annotation and docstring from source code. The API documents project is placed in the docs folder of the oneflow repository, we can generate it by the following commands. cd oneflow/docs && make dev We are looking forward to your help in adding docstring and function annotation to source code for OneFlow Python API . Review \u00b6 We are committed to building OneFlow into a basic software product full of industrial beauty with the mentality of creating artworks. This process is also inseparable with code review from open source community. We are looking forward to your submission of more issues, pull requests and reviews in each repository. We are also looking forward to your participation.","title":"Contribute to OneFlow"},{"location":"contribute/intro.html#grow-up-with-oneflow","text":"Contributing code, adding API docstring, fixing bugs and submiting issues, reviewing code ... OneFlow looks forward to your help in every way. You can participate in OneFlow's open source project by choosing one or more of the following, depending on your situation. Contribute code Contribute test cases Improve documentation Review pull requests from others Recommend OneFlow to people who need it Participate in OneFlow's group discussion \u2026\u2026 All the things you can think of to make OneFlow better","title":"Grow up with OneFlow"},{"location":"contribute/intro.html#contribute-code","text":"Our main open source code repository are\uff1a oneflow : It contains codes of OneFlow framework. OneFlow-Benchmark : It provides OneFlow deep learning benchmark examples for CV, CTR and NLP, and more models are on the way and will be provided here when ready. You can also find or create more OneFlow open source projects here .","title":"Contribute code"},{"location":"contribute/intro.html#contribute-test-cases","text":"OneFlow has reproduced and tested a lot of popular models, some related scripts and documents are placed in OneFlow-Benchmark repository. But there are more models that are not covered by OneFlow, we are looking forward to your help to bring them to OneFlow. We also strongly encourage using OneFlow in your own original research and sharing it in OneFlow-Benchmark repository.","title":"Contribute test cases"},{"location":"contribute/intro.html#improve-documentation","text":"The API documents of OneFlow are generated by sphinx to extract the annotation and docstring from source code. The API documents project is placed in the docs folder of the oneflow repository, we can generate it by the following commands. cd oneflow/docs && make dev We are looking forward to your help in adding docstring and function annotation to source code for OneFlow Python API .","title":"Improve documentation"},{"location":"contribute/intro.html#review","text":"We are committed to building OneFlow into a basic software product full of industrial beauty with the mentality of creating artworks. This process is also inseparable with code review from open source community. We are looking forward to your submission of more issues, pull requests and reviews in each repository. We are also looking forward to your participation.","title":"Review"},{"location":"extended_topics/consistent_mirrored.html","text":"In distributed training, OneFlow provides two aspects for determining the relationship between data and model. There are consistent view and mirrored view. In this article, we will introduce: The difference and applicable scenario of data parallelism and model parallelism. The characteristics of mirrored view in distributed training. The characteristics of consistent view in distributed training. Data Parallelism and Model Parallelism. \u00b6 In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism. Data Parallelism Diagram \u00b6 In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below: Model Parallelism Diagram \u00b6 In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies. Two Types of Placeholder \u00b6 In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below. Using Mirrored View in OneFlow \u00b6 Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch. Code \u00b6 In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section. Code explanation \u00b6 In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ()) Use consistent view in OneFlow \u00b6 We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely. Code Example \u00b6 In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py Code explanation \u00b6 In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) More extension \u00b6 With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"Consistent & Mirrored View"},{"location":"extended_topics/consistent_mirrored.html#data-parallelism-and-model-parallelism","text":"In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism.","title":"Data Parallelism and Model Parallelism."},{"location":"extended_topics/consistent_mirrored.html#data-parallelism-diagram","text":"In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below:","title":"Data Parallelism Diagram"},{"location":"extended_topics/consistent_mirrored.html#model-parallelism-diagram","text":"In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies.","title":"Model Parallelism Diagram"},{"location":"extended_topics/consistent_mirrored.html#two-types-of-placeholder","text":"In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below.","title":"Two Types of Placeholder"},{"location":"extended_topics/consistent_mirrored.html#using-mirrored-view-in-oneflow","text":"Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch.","title":"Using Mirrored View in OneFlow"},{"location":"extended_topics/consistent_mirrored.html#code","text":"In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section.","title":"Code"},{"location":"extended_topics/consistent_mirrored.html#code-explanation","text":"In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ())","title":"Code explanation"},{"location":"extended_topics/consistent_mirrored.html#use-consistent-view-in-oneflow","text":"We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely.","title":"Use consistent view in OneFlow"},{"location":"extended_topics/consistent_mirrored.html#code-example","text":"In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py","title":"Code Example"},{"location":"extended_topics/consistent_mirrored.html#code-explanation_1","text":"In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ())","title":"Code explanation"},{"location":"extended_topics/consistent_mirrored.html#more-extension","text":"With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"More extension"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html","text":"Convert Image Files to OFRecord Datasets \u00b6 In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset. Make OFRecord Datasets Based on Image Files \u00b6 We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset . Code Explanation \u00b6 The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved. How OFRecord Reader is Encoded \u00b6 The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes ) Training on OFRecord Dataset \u00b6 We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Convert Image Files to OFRecord Datasets"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#convert-image-files-to-ofrecord-datasets","text":"In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset.","title":"Convert Image Files to OFRecord Datasets"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#make-ofrecord-datasets-based-on-image-files","text":"We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset .","title":"Make OFRecord Datasets Based on Image Files"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#code-explanation","text":"The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved.","title":"Code Explanation"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#how-ofrecord-reader-is-encoded","text":"The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes )","title":"How OFRecord Reader is Encoded"},{"location":"extended_topics/how_to_convert_image_to_ofrecord.html#training-on-ofrecord-dataset","text":"We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Training on OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html","text":"In [data input] (... /basics_topics/data_input.md) we learned that it is usually more efficient to load data using DataLoader and related operators. Also, we learned how to use DataLoader and related operators. In article OFRecord , we learn about the storage format of OFRecord files. In this article, we will focus on the loading and generating of OneFlow's OFRecord dataset, which mainly includes: The hierarchy of OFRecord dataset Multiple ways of loading OFRecord dataset The transition between OFRecord dataset and other data formats What is OFRecord Dataset \u00b6 In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework. The Method to Load OFRecord Dataset \u00b6 We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). For more information on DataLoader and related operator usage refer to Data input . The transition between other data format data and OFRecord dataset \u00b6 According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON). The installation and launch of Spark \u00b6 At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell. Use Spark to view OFRecord dataset \u00b6 We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows The interconversion with TFRecord dataset \u00b6 we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" ) The interconversion with JSON format \u00b6 We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"Loading and Preparing OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#what-is-ofrecord-dataset","text":"In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework.","title":"What is OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-method-to-load-ofrecord-dataset","text":"We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). For more information on DataLoader and related operator usage refer to Data input .","title":"The Method to Load OFRecord Dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-transition-between-other-data-format-data-and-ofrecord-dataset","text":"According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON).","title":"The transition between other data format data and OFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-installation-and-launch-of-spark","text":"At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell.","title":"The installation and launch of Spark"},{"location":"extended_topics/how_to_make_ofdataset.html#use-spark-to-view-ofrecord-dataset","text":"We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows","title":"Use Spark to view OFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-interconversion-with-tfrecord-dataset","text":"we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" )","title":"The interconversion with TFRecord dataset"},{"location":"extended_topics/how_to_make_ofdataset.html#the-interconversion-with-json-format","text":"We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"The interconversion with JSON format"},{"location":"extended_topics/job_function_define_call.html","text":"The Definition and Call of Job Function \u00b6 In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function The Relationship Between Job Function and Running Process of OneFlow \u00b6 The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below. The Definition of Job Function \u00b6 We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The Parameters of oneflow.global_function \u00b6 oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py PlaceHolder \u00b6 Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph. The Return Value of the Job Function \u00b6 The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples. The Call of Job Function \u00b6 OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Definition and Call of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-definition-and-call-of-job-function","text":"In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function","title":"The Definition and Call of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-relationship-between-job-function-and-running-process-of-oneflow","text":"The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below.","title":"The Relationship Between Job Function and Running Process of OneFlow"},{"location":"extended_topics/job_function_define_call.html#the-definition-of-job-function","text":"We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss","title":"The Definition of Job Function"},{"location":"extended_topics/job_function_define_call.html#the-parameters-of-oneflowglobal_function","text":"oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py","title":"The Parameters of oneflow.global_function"},{"location":"extended_topics/job_function_define_call.html#placeholder","text":"Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph.","title":"PlaceHolder"},{"location":"extended_topics/job_function_define_call.html#the-return-value-of-the-job-function","text":"The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples.","title":"The Return Value of the Job Function"},{"location":"extended_topics/job_function_define_call.html#the-call-of-job-function","text":"OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Call of Job Function"},{"location":"extended_topics/model_mixed_parallel.html","text":"Features of Parallelism in OneFlow \u00b6 In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism. Network Logical Diagram in Model Training \u00b6 We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point. The Features of Parallelism in Consistent View \u00b6 Data Parallelism \u00b6 We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data. Model parallelism \u00b6 In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue. Choose the optimal parallelism method \u00b6 The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode. Hybrid Parallelism Example: \u00b6 Code \u00b6 In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\" Code explanation \u00b6 The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks. Pipelining Example \u00b6 Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining. Code \u00b6 Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details. Code Explanation \u00b6 There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Features of Parallelism in OneFlow"},{"location":"extended_topics/model_mixed_parallel.html#features-of-parallelism-in-oneflow","text":"In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism.","title":"Features of Parallelism in OneFlow"},{"location":"extended_topics/model_mixed_parallel.html#network-logical-diagram-in-model-training","text":"We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point.","title":"Network Logical Diagram in Model Training"},{"location":"extended_topics/model_mixed_parallel.html#the-features-of-parallelism-in-consistent-view","text":"","title":"The Features of Parallelism in Consistent View"},{"location":"extended_topics/model_mixed_parallel.html#data-parallelism","text":"We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data.","title":"Data Parallelism"},{"location":"extended_topics/model_mixed_parallel.html#model-parallelism","text":"In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue.","title":"Model parallelism"},{"location":"extended_topics/model_mixed_parallel.html#choose-the-optimal-parallelism-method","text":"The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode.","title":"Choose the optimal parallelism method"},{"location":"extended_topics/model_mixed_parallel.html#hybrid-parallelism-example","text":"","title":"Hybrid Parallelism Example:"},{"location":"extended_topics/model_mixed_parallel.html#code","text":"In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\"","title":"Code"},{"location":"extended_topics/model_mixed_parallel.html#code-explanation","text":"The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks.","title":"Code explanation"},{"location":"extended_topics/model_mixed_parallel.html#pipelining-example","text":"Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining.","title":"Pipelining Example"},{"location":"extended_topics/model_mixed_parallel.html#code_1","text":"Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details.","title":"Code"},{"location":"extended_topics/model_mixed_parallel.html#code-explanation_1","text":"There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Code Explanation"},{"location":"extended_topics/ofrecord.html","text":"Deep Learning applications need complex multi-stage data preprocessing pipeline, the first step of data pipeline is data loading. OneFlow supports multiple data formats in data loading, among which OFRecord format is the native data format of OneFlow. The data format definition of OFRecord is similar to TFRecord of Tensorflow. Users familiar with TFRecord can start with OneFlow's OFRecord quickly. Key points of this article\uff1a The data type used in OFRecord How to convert data to OFRecord object and serialize it The file format of OFRecord It should be helpful for users to learn how to make ofdataset after learning the above contents. Data Types of OFRecord \u00b6 Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level. Convert Data into Feature Format \u00b6 Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value )) Creating and Serializing OFRecord Object \u00b6 In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format. OFRecord Format File \u00b6 According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes ) Code \u00b6 The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details. Write OFRecord Object to File \u00b6 In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py Read data from OFRecord file \u00b6 The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"The OFRecord Data Format"},{"location":"extended_topics/ofrecord.html#data-types-of-ofrecord","text":"Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level.","title":"Data Types of OFRecord"},{"location":"extended_topics/ofrecord.html#convert-data-into-feature-format","text":"Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value ))","title":"Convert Data into Feature Format"},{"location":"extended_topics/ofrecord.html#creating-and-serializing-ofrecord-object","text":"In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format.","title":"Creating and Serializing OFRecord Object"},{"location":"extended_topics/ofrecord.html#ofrecord-format-file","text":"According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes )","title":"OFRecord Format File"},{"location":"extended_topics/ofrecord.html#code","text":"The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details.","title":"Code"},{"location":"extended_topics/ofrecord.html#write-ofrecord-object-to-file","text":"In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py","title":"Write OFRecord Object to File"},{"location":"extended_topics/ofrecord.html#read-data-from-ofrecord-file","text":"The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"Read data from OFRecord file"},{"location":"extended_topics/oneflow_convert_tools.html","text":"oneflow_convert_tools \u00b6 oneflow_onnx \u00b6 Introduction \u00b6 oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product. Environment Dependencies \u00b6 User's Environment Configuration \u00b6 python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4 Installation \u00b6 Method 1 \u00b6 pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install Usage \u00b6 Please refer to Examples Related Documents \u00b6 OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples nchw2nhwc_tool \u00b6 Introduction \u00b6 This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information save_serving_tool \u00b6 Introduction \u00b6 This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"OneFlow And ONNX Convert"},{"location":"extended_topics/oneflow_convert_tools.html#oneflow_convert_tools","text":"","title":"oneflow_convert_tools"},{"location":"extended_topics/oneflow_convert_tools.html#oneflow_onnx","text":"","title":"oneflow_onnx"},{"location":"extended_topics/oneflow_convert_tools.html#introduction","text":"oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product.","title":"Introduction"},{"location":"extended_topics/oneflow_convert_tools.html#environment-dependencies","text":"","title":"Environment Dependencies"},{"location":"extended_topics/oneflow_convert_tools.html#users-environment-configuration","text":"python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4","title":"User's Environment Configuration"},{"location":"extended_topics/oneflow_convert_tools.html#installation","text":"","title":"Installation"},{"location":"extended_topics/oneflow_convert_tools.html#method-1","text":"pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install","title":"Method 1"},{"location":"extended_topics/oneflow_convert_tools.html#usage","text":"Please refer to Examples","title":"Usage"},{"location":"extended_topics/oneflow_convert_tools.html#related-documents","text":"OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples","title":"Related Documents"},{"location":"extended_topics/oneflow_convert_tools.html#nchw2nhwc_tool","text":"","title":"nchw2nhwc_tool"},{"location":"extended_topics/oneflow_convert_tools.html#introduction_1","text":"This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information","title":"Introduction"},{"location":"extended_topics/oneflow_convert_tools.html#save_serving_tool","text":"","title":"save_serving_tool"},{"location":"extended_topics/oneflow_convert_tools.html#introduction_2","text":"This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"Introduction"},{"location":"extended_topics/watch_watch_diff.html","text":"How to Obtain Runtime Data \u00b6 OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime. Using Guidance \u00b6 To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff . Use watch to Obtain the Data when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ] Code Explanation \u00b6 In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements. Use watch_diff to Obtain Gradient when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32 Code Explanation \u00b6 In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Obtain Runtime Data"},{"location":"extended_topics/watch_watch_diff.html#how-to-obtain-runtime-data","text":"OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime.","title":"How to Obtain Runtime Data"},{"location":"extended_topics/watch_watch_diff.html#using-guidance","text":"To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff .","title":"Using Guidance"},{"location":"extended_topics/watch_watch_diff.html#use-watch-to-obtain-the-data-when-running","text":"The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ]","title":"Use watch to Obtain the Data when Running"},{"location":"extended_topics/watch_watch_diff.html#code-explanation","text":"In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements.","title":"Code Explanation"},{"location":"extended_topics/watch_watch_diff.html#use-watch_diff-to-obtain-gradient-when-running","text":"The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32","title":"Use watch_diff to Obtain Gradient when Running"},{"location":"extended_topics/watch_watch_diff.html#code-explanation_1","text":"In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Code Explanation"},{"location":"quick_start/install.html","text":"Install OneFlow Stable Version \u00b6 Install the latest stable version of OneFlow with CUDA support using the following command: python3 -m pip install -f https://release.oneflow.info oneflow==0.4.0+cu102 Install the latest version of the OneFlow master branch using the following command (not recommended for production environments): python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/cu102 If you are informed that the corresponding version cannot be found, please try upgrading pip : python3 -m pip install --upgrade pip #--user Chinese users can use the domestic mirror to accelerate: python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Detailed instructions can be found in the pypi mirror help \u3002 System Requirements: Python >= 3.6 CUDA driver requirements are available in the OneFlow source code repository README Build from source \u00b6 If you want to install OneFlow by building from source, please refer to README . Also refer to Troubleshooting for common issues you might encounter when compiling and running OneFlow. Install OneFlow with legacy CUDA support \u00b6 To install OneFlow with legacy CUDA support, run one of the following command: Stable: python3 -m pip install --find-links https://release.oneflow.info oneflow==0.4.0+[PLATFORM] Nightly: python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/[PLATFORM] All available [PLATFORM] : Platform CUDA Driver Version Supported GPUs cu112 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu111 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu110, cu110_xla >= 450.36.06 GTX 10xx, RTX 20xx, A100 cu102, cu102_xla >= 440.33 GTX 10xx, RTX 20xx cu101, cu101_xla >= 418.39 GTX 10xx, RTX 20xx cu100, cu100_xla >= 410.48 GTX 10xx, RTX 20xx cpu N/A N/A QQ channel \u00b6 If you encounter any problems during the installation and want for help, please join the QQ channel or submit issues on Github . QQ channel ID: 331883 or scan QR code below","title":"Installation"},{"location":"quick_start/install.html#install-oneflow-stable-version","text":"Install the latest stable version of OneFlow with CUDA support using the following command: python3 -m pip install -f https://release.oneflow.info oneflow==0.4.0+cu102 Install the latest version of the OneFlow master branch using the following command (not recommended for production environments): python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/cu102 If you are informed that the corresponding version cannot be found, please try upgrading pip : python3 -m pip install --upgrade pip #--user Chinese users can use the domestic mirror to accelerate: python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Detailed instructions can be found in the pypi mirror help \u3002 System Requirements: Python >= 3.6 CUDA driver requirements are available in the OneFlow source code repository README","title":"Install OneFlow Stable Version"},{"location":"quick_start/install.html#build-from-source","text":"If you want to install OneFlow by building from source, please refer to README . Also refer to Troubleshooting for common issues you might encounter when compiling and running OneFlow.","title":"Build from source"},{"location":"quick_start/install.html#install-oneflow-with-legacy-cuda-support","text":"To install OneFlow with legacy CUDA support, run one of the following command: Stable: python3 -m pip install --find-links https://release.oneflow.info oneflow==0.4.0+[PLATFORM] Nightly: python3 -m pip install oneflow -f https://staging.oneflow.info/branch/master/[PLATFORM] All available [PLATFORM] : Platform CUDA Driver Version Supported GPUs cu112 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu111 >= 450.80.02 GTX 10xx, RTX 20xx, A100, RTX 30xx cu110, cu110_xla >= 450.36.06 GTX 10xx, RTX 20xx, A100 cu102, cu102_xla >= 440.33 GTX 10xx, RTX 20xx cu101, cu101_xla >= 418.39 GTX 10xx, RTX 20xx cu100, cu100_xla >= 410.48 GTX 10xx, RTX 20xx cpu N/A N/A","title":"Install OneFlow with legacy CUDA support"},{"location":"quick_start/install.html#qq-channel","text":"If you encounter any problems during the installation and want for help, please join the QQ channel or submit issues on Github . QQ channel ID: 331883 or scan QR code below","title":"QQ channel"},{"location":"quick_start/lenet_mnist.html","text":"This article covers topics below: Configuring the hardware and software environment using the OneFlow interface Define models using OneFlow's interface Model training with train type How to save/load model Use the predict type for model evaluation Using predict type for image recognition This article demonstrates the key steps of how to train a LeNet model with MNIST dataset using OneFlow. The full example code is attached at the end of article. You can see the effects of each script by running the following commands (The script operation rely on the default GPU No.0 on your machine. If you install the CPU version of OneFlow, the script will automatically call the CPU for training/evaluation). First of all, clone the documentation repository and switch to the corresponding path: git clone https://github.com/Oneflow-Inc/oneflow-documentation.git cd oneflow-documentation/en/docs/code/quick_start/ Training model python lenet_train.py The commands above will train a model with MNIST dataset and save it. Output\uff1a File mnist.npz already exist, path: ./mnist.npz 5.9947124 1.0865117 0.5317516 0.20937675 0.26428983 0.21764673 0.23443426 ... A trained model is the prerequisite of lenet_eval.py and lenet_test.py . We can directly download a trained model to skip the training progress: #change directory to: en/docs/code/quick_start/ wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip Evaluation python lenet_eval.py The command above uses the MNIST's testing set to evaluate the trained model and print out the accuracy. Output\uff1a File mnist.npz already exist, path: ./mnist.npz accuracy: 99.4% Image recognition python lenet_test.py ./9.png # Output\uff1aprediction: 9 The above command will use the trained model to predict the content of file \"9.png\". We can also download and verify more from prepared images . Introduction of MNIST Dataset \u00b6 MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/ Define Training Model \u00b6 Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model. Implementation of Job Function for Training \u00b6 OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function . global_function Decorator \u00b6 oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object. Setup Optimizer \u00b6 We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried). Calling the Job Function and Get Results \u00b6 We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function. Example on Single Return Value \u00b6 The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations. Example on Multiple Return Values \u00b6 In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model. Synchronous and Asynchronous Call \u00b6 All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function . Model Initialization, Saving and Loading \u00b6 Model Initialization and Saving \u00b6 The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters. Model Loading \u00b6 During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ... Evaluation of Model \u00b6 The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed. Job Function for Evaluation \u00b6 @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values. Process of Evaluation \u00b6 The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set. Image Prediction \u00b6 After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main () Code \u00b6 Model training \u00b6 Script: lenet_train.py Model evaluation \u00b6 Script: lenet_eval.py Saved model: lenet_models_1.zip Digits prediction \u00b6 Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Recognition of MNIST Handwritten Digits"},{"location":"quick_start/lenet_mnist.html#introduction-of-mnist-dataset","text":"MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/","title":"Introduction of MNIST Dataset"},{"location":"quick_start/lenet_mnist.html#define-training-model","text":"Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model.","title":"Define Training Model"},{"location":"quick_start/lenet_mnist.html#implementation-of-job-function-for-training","text":"OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function .","title":"Implementation of Job Function for Training"},{"location":"quick_start/lenet_mnist.html#global_function-decorator","text":"oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object.","title":"global_function Decorator"},{"location":"quick_start/lenet_mnist.html#setup-optimizer","text":"We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried).","title":"Setup Optimizer"},{"location":"quick_start/lenet_mnist.html#calling-the-job-function-and-get-results","text":"We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function.","title":"Calling the Job Function and Get Results"},{"location":"quick_start/lenet_mnist.html#example-on-single-return-value","text":"The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations.","title":"Example on Single Return Value"},{"location":"quick_start/lenet_mnist.html#example-on-multiple-return-values","text":"In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model.","title":"Example on Multiple Return Values"},{"location":"quick_start/lenet_mnist.html#synchronous-and-asynchronous-call","text":"All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function .","title":"Synchronous and Asynchronous Call"},{"location":"quick_start/lenet_mnist.html#model-initialization-saving-and-loading","text":"","title":"Model Initialization, Saving and Loading"},{"location":"quick_start/lenet_mnist.html#model-initialization-and-saving","text":"The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters.","title":"Model Initialization and Saving"},{"location":"quick_start/lenet_mnist.html#model-loading","text":"During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ...","title":"Model Loading"},{"location":"quick_start/lenet_mnist.html#evaluation-of-model","text":"The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed.","title":"Evaluation of Model"},{"location":"quick_start/lenet_mnist.html#job-function-for-evaluation","text":"@flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values.","title":"Job Function for Evaluation"},{"location":"quick_start/lenet_mnist.html#process-of-evaluation","text":"The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set.","title":"Process of Evaluation"},{"location":"quick_start/lenet_mnist.html#image-prediction","text":"After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main ()","title":"Image Prediction"},{"location":"quick_start/lenet_mnist.html#code","text":"","title":"Code"},{"location":"quick_start/lenet_mnist.html#model-training","text":"Script: lenet_train.py","title":"Model training"},{"location":"quick_start/lenet_mnist.html#model-evaluation","text":"Script: lenet_eval.py Saved model: lenet_models_1.zip","title":"Model evaluation"},{"location":"quick_start/lenet_mnist.html#digits-prediction","text":"Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Digits prediction"},{"location":"quick_start/quickstart_in_3_min.html","text":"This article introduces how to quickly get start with OneFlow. We can complete a full neural network training process just in 3 minutes. Example \u00b6 With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/master/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow. Code Explanation \u00b6 The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark . FAQ \u00b6 Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"Quick Start in 3 Minutes"},{"location":"quick_start/quickstart_in_3_min.html#example","text":"With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/master/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow.","title":"Example"},{"location":"quick_start/quickstart_in_3_min.html#code-explanation","text":"The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark .","title":"Code Explanation"},{"location":"quick_start/quickstart_in_3_min.html#faq","text":"Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"FAQ"}]}